{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4304e342-c511-4a39-8009-fb275cc2dada",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "507ec23c-de47-4faf-afe0-b70969eaacb2",
     "showTitle": false,
     "title": "--i18n-626549c9-0309-43a3-baec-ae0fdcaa35ca"
    }
   },
   "source": [
    "\n",
    "\n",
    "# Partitioning\n",
    "##### Objectives\n",
    "1. Get partitions and cores\n",
    "1. Repartition DataFrames\n",
    "1. Configure default shuffle partitions\n",
    "\n",
    "##### Methods\n",
    "- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html\" target=\"_blank\">DataFrame</a>: **`repartition`**, **`coalesce`**, **`rdd.getNumPartitions`**\n",
    "- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkConf.html\" target=\"_blank\">SparkConf</a>: **`get`**, **`set`**\n",
    "- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.SparkSession.html\" target=\"_blank\">SparkSession</a>: **`spark.sparkContext.defaultParallelism`**\n",
    "\n",
    "##### SparkConf Parameters\n",
    "- **`spark.sql.shuffle.partitions`**, **`spark.sql.adaptive.enabled`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62005245-847a-4406-af99-321c347ec473",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nPython interpreter will be restarted.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting the learning environment:\n| No action taken\n\nSkipping install of existing datasets to \"dbfs:/mnt/dbacademy-datasets/apache-spark-programming-with-databricks/v03\"\n\nValidating the locally installed datasets:\n| listing local files...(3 seconds)\n| validation completed...(3 seconds total)\n\nCreating & using the schema \"muaazkhurshid_q1yg_da_asp\" in the catalog \"spark_catalog\"...(0 seconds)\n\nPredefined tables in \"muaazkhurshid_q1yg_da_asp\":\n| -none-\n\nPredefined paths variables:\n| DA.paths.working_dir: dbfs:/mnt/dbacademy-users/muaazkhurshid@uni-koblenz.de/apache-spark-programming-with-databricks\n| DA.paths.user_db:     dbfs:/mnt/dbacademy-users/muaazkhurshid@uni-koblenz.de/apache-spark-programming-with-databricks/database.db\n| DA.paths.datasets:    dbfs:/mnt/dbacademy-datasets/apache-spark-programming-with-databricks/v03\n| DA.paths.checkpoints: dbfs:/mnt/dbacademy-users/muaazkhurshid@uni-koblenz.de/apache-spark-programming-with-databricks/_checkpoints\n\nSetup completed (5 seconds)\n\nPredefined tables in \"muaazkhurshid_q1yg_da_asp\":\n| -none-\n\nPredefined paths variables:\n| DA.paths.working_dir: dbfs:/mnt/dbacademy-users/muaazkhurshid@uni-koblenz.de/apache-spark-programming-with-databricks\n| DA.paths.user_db:     dbfs:/mnt/dbacademy-users/muaazkhurshid@uni-koblenz.de/apache-spark-programming-with-databricks/database.db\n| DA.paths.datasets:    dbfs:/mnt/dbacademy-datasets/apache-spark-programming-with-databricks/v03\n| DA.paths.checkpoints: dbfs:/mnt/dbacademy-users/muaazkhurshid@uni-koblenz.de/apache-spark-programming-with-databricks/_checkpoints\n| DA.paths.sales:       dbfs:/mnt/dbacademy-datasets/apache-spark-programming-with-databricks/v03/ecommerce/sales/sales.delta\n| DA.paths.users:       dbfs:/mnt/dbacademy-datasets/apache-spark-programming-with-databricks/v03/ecommerce/users/users.delta\n| DA.paths.events:      dbfs:/mnt/dbacademy-datasets/apache-spark-programming-with-databricks/v03/ecommerce/events/events.delta\n| DA.paths.products:    dbfs:/mnt/dbacademy-datasets/apache-spark-programming-with-databricks/v03/products/products.delta\n\nSetup completed (5 seconds)\n"
     ]
    }
   ],
   "source": [
    "%run ../Includes/Classroom-Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e02b3c63-f88d-4ea7-b03d-8ff00e23c956",
     "showTitle": false,
     "title": "--i18n-17830688-7910-42d3-a253-c688ff562e9c"
    }
   },
   "source": [
    "\n",
    "\n",
    "### Get partitions and cores\n",
    "\n",
    "Use the **`rdd`** method **`getNumPartitions`** to get the number of DataFrame partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d94f891f-4676-4218-a326-408b551282ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[4]: 4"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").load(DA.paths.events)\n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc1d5c42-01af-4607-bdf0-e8db645d3f52",
     "showTitle": false,
     "title": "--i18n-845301f4-e139-4dff-a6ad-c78d3b8aceee"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "Access **`SparkContext`** through **`SparkSession`** to get the number of cores or slots.\n",
    "\n",
    "Use the **`defaultParallelism`** attribute to get the number of cores in a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99ea3a43-f128-44e5-9cc5-f3788fd68024",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(spark.sparkContext.defaultParallelism)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec3e9239-10e3-42f7-8041-2845eb3977d5",
     "showTitle": false,
     "title": "--i18n-2b54cb1e-fff0-432d-9a26-dc26efe90a24"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "**`SparkContext`** is also provided in Databricks notebooks as the variable **`sc`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebd77690-e4d0-451a-b419-343696ca311d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(sc.defaultParallelism)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8a8cc76-3bbb-4f12-b3bf-bb4535ed5a37",
     "showTitle": false,
     "title": "--i18n-7e13bd07-e839-45f5-b727-d9763e7e83d6"
    }
   },
   "source": [
    "\n",
    "\n",
    "### Repartition DataFrame\n",
    "\n",
    "There are two methods available to repartition a DataFrame: **`repartition`** and **`coalesce`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1019b410-63fd-4f9b-8958-f648c2bcc4d0",
     "showTitle": false,
     "title": "--i18n-728b9e76-ac70-4003-bc8c-dcde137155f1"
    }
   },
   "source": [
    "\n",
    "\n",
    "#### **`repartition`**\n",
    "Returns a new DataFrame that has exactly **`n`** partitions.\n",
    "\n",
    "- Wide transformation\n",
    "- Pro: Evenly balances partition sizes  \n",
    "- Con: Requires shuffling all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6efb620c-2663-4904-9383-d8354040efe6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "repartitioned_df = df.repartition(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78d5c44b-d882-4051-b14b-e15ddc393d16",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[8]: 8"
     ]
    }
   ],
   "source": [
    "repartitioned_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d7c87ba-4ab6-4ced-b05f-7b41255823ab",
     "showTitle": false,
     "title": "--i18n-1d5bbde0-3619-44ff-8fc6-b62a794b6e94"
    }
   },
   "source": [
    "\n",
    "\n",
    "#### **`coalesce`**\n",
    "Returns a new DataFrame that has exactly **`n`** partitions, when fewer partitions are requested.\n",
    "\n",
    "If a larger number of partitions is requested, it will stay at the current number of partitions.\n",
    "\n",
    "- Narrow transformation, some partitions are effectively concatenated\n",
    "- Pro: Requires no shuffling\n",
    "- Cons:\n",
    "  - Is not able to increase # partitions\n",
    "  - Can result in uneven partition sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d03dd527-5112-4876-bda0-6f4591d5c211",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "coalesce_df = df.coalesce(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7eeac24-499f-4232-904b-f0e418aa90eb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[10]: 4"
     ]
    }
   ],
   "source": [
    "coalesce_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4f2843d-43d2-4094-9bf4-1d16d7896810",
     "showTitle": false,
     "title": "--i18n-5724d4ef-45d8-44db-a703-d6827dbfde91"
    }
   },
   "source": [
    "\n",
    "\n",
    "### Configure default shuffle partitions\n",
    "\n",
    "Use the SparkSession's **`conf`** attribute to get and set dynamic Spark configuration properties. The **`spark.sql.shuffle.partitions`** property determines the number of partitions that result from a shuffle. Let's check its default value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8ec3da4-0980-448e-9038-0ae2bee9eb04",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[11]: '200'"
     ]
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05b85baf-9d90-462b-b0c7-df1e2deabdd5",
     "showTitle": false,
     "title": "--i18n-c67a0a9d-bb97-43a8-aca5-275775e326b7"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "Assuming that the data set isn't too large, you could configure the default number of shuffle partitions to match the number of cores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c5d3bd1-e667-463e-beb0-c561230ae955",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", spark.sparkContext.defaultParallelism)\n",
    "print(spark.conf.get(\"spark.sql.shuffle.partitions\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f11ec2eb-fc5b-4392-8608-2868473f4009",
     "showTitle": false,
     "title": "--i18n-dae30e85-6039-437c-9cb8-cf8cd16c84b9"
    }
   },
   "source": [
    "\n",
    "\n",
    "### Partitioning Guidelines\n",
    "- Make the number of partitions a multiple of the number of cores\n",
    "- Target a partition size of ~200MB\n",
    "- Size default shuffle partitions by dividing largest shuffle stage input by the target partition size (e.g., 4TB / 200MB = 20,000 shuffle partition count)\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_note_32.png\" alt=\"Note\"> When writing a DataFrame to storage, the number of DataFrame partitions determines the number of data files written. (This assumes that <a href=\"https://sparkbyexamples.com/apache-hive/hive-partitions-explained-with-examples/\" target=\"_blank\">Hive partitioning</a> is not used for the data in storage. A discussion of DataFrame partitioning vs Hive partitioning is beyond the scope of this class.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "beed90d0-9d01-45aa-a45f-f001c18ae53b",
     "showTitle": false,
     "title": "--i18n-39e11a07-1a9f-4d84-a119-8b1dcbe33405"
    }
   },
   "source": [
    "\n",
    "\n",
    "### Adaptive Query Execution\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/aspwd/partitioning_aqe.png\" width=\"60%\" />\n",
    "\n",
    "In Spark 3, <a href=\"https://spark.apache.org/docs/latest/sql-performance-tuning.html#adaptive-query-execution\" target=\"_blank\">AQE</a> is now able to <a href=\"https://databricks.com/blog/2020/05/29/adaptive-query-execution-speeding-up-spark-sql-at-runtime.html\" target=\"_blank\"> dynamically coalesce shuffle partitions</a> at runtime. This means that you can set **`spark.sql.shuffle.partitions`** based on the largest data set your application processes and allow AQE to reduce the number of partitions automatically when there is less data to process.\n",
    "\n",
    "The **`spark.sql.adaptive.enabled`** configuration option controls whether AQE is turned on/off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b29d1c7-de93-4bcf-9e6a-04afbeb81c52",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[13]: 'true'"
     ]
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.adaptive.enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c534ef58-7cf9-425f-9b69-652cb9d4b945",
     "showTitle": false,
     "title": "--i18n-ed5dae82-9a3a-42b0-990f-2560f8ee59fe"
    }
   },
   "source": [
    "\n",
    "\n",
    "### Clean up classroom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa85fdb4-087d-4453-804b-f9d3bcb3d130",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting the learning environment:\n| dropping the schema \"muaazkhurshid_q1yg_da_asp\"...(1 seconds)\n| removing the working directory \"dbfs:/mnt/dbacademy-users/muaazkhurshid@uni-koblenz.de/apache-spark-programming-with-databricks\"...(0 seconds)\n\nValidating the locally installed datasets:\n| listing local files...(3 seconds)\n| validation completed...(3 seconds total)\n"
     ]
    }
   ],
   "source": [
    "DA.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c77e6cf8-d13c-44e4-b0ee-741a0c68d3dc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "&copy; 2023 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/>\n",
    "<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "ASP 4.2 - Partitioning",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
