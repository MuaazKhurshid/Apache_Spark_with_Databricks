{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "947cc5ba-66dc-40a2-a8b0-648bd7107100",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80997e0d-0bfc-40bd-9997-ef814adf17e8",
     "showTitle": false,
     "title": "--i18n-b5b99f22-732d-417b-a5e3-90be38f153d0"
    }
   },
   "source": [
    "\n",
    "\n",
    "# Datetime Functions\n",
    "\n",
    "##### Objectives\n",
    "1. Cast to timestamp\n",
    "2. Format datetimes\n",
    "3. Extract from timestamp\n",
    "4. Convert to date\n",
    "5. Manipulate datetimes\n",
    "\n",
    "##### Methods\n",
    "- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/column.html\" target=\"_blank\">Column</a>: **`cast`**\n",
    "- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html#datetime-functions\" target=\"_blank\">Built-In Functions</a>: **`date_format`**, **`to_date`**, **`date_add`**, **`year`**, **`month`**, **`dayofweek`**, **`minute`**, **`second`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b0411c8-2982-4226-8055-230923121086",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nPython interpreter will be restarted.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting the learning environment:\n| No action taken\n\nSkipping install of existing datasets to \"dbfs:/mnt/dbacademy-datasets/apache-spark-programming-with-databricks/v03\"\n\nValidating the locally installed datasets:\n| listing local files...(4 seconds)\n| validation completed...(4 seconds total)\n\nCreating & using the schema \"muaazkhurshid_q1yg_da_asp\" in the catalog \"spark_catalog\"...(1 seconds)\n\nPredefined tables in \"muaazkhurshid_q1yg_da_asp\":\n| -none-\n\nPredefined paths variables:\n| DA.paths.working_dir: dbfs:/mnt/dbacademy-users/muaazkhurshid@uni-koblenz.de/apache-spark-programming-with-databricks\n| DA.paths.user_db:     dbfs:/mnt/dbacademy-users/muaazkhurshid@uni-koblenz.de/apache-spark-programming-with-databricks/database.db\n| DA.paths.datasets:    dbfs:/mnt/dbacademy-datasets/apache-spark-programming-with-databricks/v03\n| DA.paths.checkpoints: dbfs:/mnt/dbacademy-users/muaazkhurshid@uni-koblenz.de/apache-spark-programming-with-databricks/_checkpoints\n\nSetup completed (25 seconds)\n\nPredefined tables in \"muaazkhurshid_q1yg_da_asp\":\n| -none-\n\nPredefined paths variables:\n| DA.paths.working_dir: dbfs:/mnt/dbacademy-users/muaazkhurshid@uni-koblenz.de/apache-spark-programming-with-databricks\n| DA.paths.user_db:     dbfs:/mnt/dbacademy-users/muaazkhurshid@uni-koblenz.de/apache-spark-programming-with-databricks/database.db\n| DA.paths.datasets:    dbfs:/mnt/dbacademy-datasets/apache-spark-programming-with-databricks/v03\n| DA.paths.checkpoints: dbfs:/mnt/dbacademy-users/muaazkhurshid@uni-koblenz.de/apache-spark-programming-with-databricks/_checkpoints\n| DA.paths.sales:       dbfs:/mnt/dbacademy-datasets/apache-spark-programming-with-databricks/v03/ecommerce/sales/sales.delta\n| DA.paths.users:       dbfs:/mnt/dbacademy-datasets/apache-spark-programming-with-databricks/v03/ecommerce/users/users.delta\n| DA.paths.events:      dbfs:/mnt/dbacademy-datasets/apache-spark-programming-with-databricks/v03/ecommerce/events/events.delta\n| DA.paths.products:    dbfs:/mnt/dbacademy-datasets/apache-spark-programming-with-databricks/v03/products/products.delta\n\nSetup completed (25 seconds)\n"
     ]
    }
   ],
   "source": [
    "%run ../Includes/Classroom-Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6112622b-a2d8-4ef0-8228-d6cdb5982d4f",
     "showTitle": false,
     "title": "--i18n-a972cc76-7c1e-4a1f-880a-053dedbb3554"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "Let's use a subset of the BedBricks events dataset to practice working with date times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb2b3485-f8ab-45fc-8a5c-f4ecbc887e17",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = spark.read.format(\"delta\").load(DA.paths.events).select(\"user_id\", col(\"event_timestamp\").alias(\"timestamp\"))\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "761a924c-93f9-4564-a7be-b8a7d2614ad4",
     "showTitle": false,
     "title": "--i18n-3961f384-8390-42e6-8908-1a076590790e"
    }
   },
   "source": [
    "\n",
    "\n",
    "### Built-In Functions: Date Time Functions\n",
    "Here are a few built-in functions to manipulate dates and times in Spark.\n",
    "\n",
    "| Method | Description |\n",
    "| --- | --- |\n",
    "| **`add_months`** | Returns the date that is numMonths after startDate |\n",
    "| **`current_timestamp`** | Returns the current timestamp at the start of query evaluation as a timestamp column |\n",
    "| **`date_format`** | Converts a date/timestamp/string to a value of string in the format specified by the date format given by the second argument. |\n",
    "| **`dayofweek`** | Extracts the day of the month as an integer from a given date/timestamp/string |\n",
    "| **`from_unixtime`** | Converts the number of seconds from unix epoch (1970-01-01 00:00:00 UTC) to a string representing the timestamp of that moment in the current system time zone in the yyyy-MM-dd HH:mm:ss format |\n",
    "| **`minute`** | Extracts the minutes as an integer from a given date/timestamp/string. |\n",
    "| **`unix_timestamp`** | Converts time string with given pattern to Unix timestamp (in seconds) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5ec6b76-2152-4343-99c1-62acc9f2cd54",
     "showTitle": false,
     "title": "--i18n-e8347945-63e6-44fb-9031-92ebd8dfc3c1"
    }
   },
   "source": [
    "\n",
    "\n",
    "### Cast to Timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7997ab5c-0234-4991-821d-13f8866f8a1d",
     "showTitle": false,
     "title": "--i18n-233b3972-75ab-45a3-ba7d-57716f56f852"
    }
   },
   "source": [
    "\n",
    "\n",
    "#### **`cast()`**\n",
    "Casts column to a different data type, specified using string representation or DataType."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa0e3265-2aaa-4fe1-aa6f-bb820d61c3b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "timestamp_df = df.withColumn(\"timestamp\", (col(\"timestamp\") / 1e6).cast(\"timestamp\"))\n",
    "# display(timestamp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2d55a75-a4cc-4e1c-bc2a-820fa7b77627",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "timestamp_df = df.withColumn(\"timestamp\", (col(\"timestamp\") / 1e6).cast(TimestampType()))\n",
    "# display(timestamp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dee8ff3b-3824-46ff-9dae-10e470dac903",
     "showTitle": false,
     "title": "--i18n-560730f5-f0e9-46e1-b309-3645a43b8fad"
    }
   },
   "source": [
    "\n",
    "\n",
    "### Datetime Patterns for Formatting and Parsing\n",
    "There are several common scenarios for datetime usage in Spark:\n",
    "\n",
    "- CSV/JSON datasources use the pattern string for parsing and formatting datetime content.\n",
    "- Datetime functions related to convert StringType to/from DateType or TimestampType e.g. **`unix_timestamp`**, **`date_format`**, **`from_unixtime`**, **`to_date`**, **`to_timestamp`**, etc.\n",
    "\n",
    "Spark uses <a href=\"https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\" target=\"_blank\">pattern letters for date and timestamp parsing and formatting</a>. A subset of these patterns are shown below.\n",
    "\n",
    "| Symbol | Meaning         | Presentation | Examples               |\n",
    "| ------ | --------------- | ------------ | ---------------------- |\n",
    "| G      | era             | text         | AD; Anno Domini        |\n",
    "| y      | year            | year         | 2020; 20               |\n",
    "| D      | day-of-year     | number(3)    | 189                    |\n",
    "| M/L    | month-of-year   | month        | 7; 07; Jul; July       |\n",
    "| d      | day-of-month    | number(3)    | 28                     |\n",
    "| Q/q    | quarter-of-year | number/text  | 3; 03; Q3; 3rd quarter |\n",
    "| E      | day-of-week     | text         | Tue; Tuesday           |\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_warn_32.png\" alt=\"Warning\"> Spark's handling of dates and timestamps changed in version 3.0, and the patterns used for parsing and formatting these values changed as well. For a discussion of these changes, please reference <a href=\"https://databricks.com/blog/2020/07/22/a-comprehensive-look-at-dates-and-timestamps-in-apache-spark-3-0.html\" target=\"_blank\">this Databricks blog post</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3054eaec-e1b0-4644-91e5-255cefb8824f",
     "showTitle": false,
     "title": "--i18n-ddbc5ab6-a545-4331-8dc2-0259d3fcc5ab"
    }
   },
   "source": [
    "\n",
    "\n",
    "### Format date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5bb8fc3-b192-41d3-8971-6732d6069ac4",
     "showTitle": false,
     "title": "--i18n-6886382d-e6d3-42e8-9f3d-9b5941fc0a94"
    }
   },
   "source": [
    "\n",
    "\n",
    "#### **`date_format()`**\n",
    "Converts a date/timestamp/string to a string formatted with the given date time pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e99778c-f2b7-427d-af69-8cc21f39bf05",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format\n",
    "\n",
    "formatted_df = (timestamp_df\n",
    "                .withColumn(\"date string\", date_format(\"timestamp\", \"MMMM dd, yyyy\"))\n",
    "                .withColumn(\"time string\", date_format(\"timestamp\", \"HH:mm:ss.SSSSSS\"))\n",
    "               )\n",
    "# display(formatted_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6b90bf3-2355-425a-883e-0cd0a4e2d69f",
     "showTitle": false,
     "title": "--i18n-f4504afe-b8ad-48b2-a703-880d215b99d1"
    }
   },
   "source": [
    "\n",
    "\n",
    "### Extract datetime attribute from timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a305c3a6-2ee9-424f-aef7-b1fbc15cbd42",
     "showTitle": false,
     "title": "--i18n-7730541d-3880-4a43-995a-e3833a623a02"
    }
   },
   "source": [
    "\n",
    "\n",
    "#### **`year`**\n",
    "Extracts the year as an integer from a given date/timestamp/string.\n",
    "\n",
    "##### Similar methods: **`month`**, **`dayofweek`**, **`minute`**, **`second`**, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "913b97f3-e0b1-4bb3-9fc3-d1983a03349b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, dayofweek, minute, second\n",
    "\n",
    "datetime_df = (timestamp_df\n",
    "               .withColumn(\"year\", year(col(\"timestamp\")))\n",
    "               .withColumn(\"month\", month(col(\"timestamp\")))\n",
    "               .withColumn(\"dayofweek\", dayofweek(col(\"timestamp\")))\n",
    "               .withColumn(\"minute\", minute(col(\"timestamp\")))\n",
    "               .withColumn(\"second\", second(col(\"timestamp\")))\n",
    "              )\n",
    "# display(datetime_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0403a7c-3793-44a2-8e75-7434f242050d",
     "showTitle": false,
     "title": "--i18n-1e9c587a-0223-42ab-a201-8a3149ba1172"
    }
   },
   "source": [
    "\n",
    "\n",
    "### Convert to Date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f49e69db-64bf-46e4-9e35-c0d8b97524a1",
     "showTitle": false,
     "title": "--i18n-fcfd617a-c062-489d-b6f5-efe8e31005eb"
    }
   },
   "source": [
    "\n",
    "\n",
    "#### **`to_date`**\n",
    "Converts the column into DateType by casting rules to DateType."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "228f38bf-b8a2-46cc-8724-e1c08dea457a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "date_df = timestamp_df.withColumn(\"date\", to_date(col(\"timestamp\")))\n",
    "# display(date_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d7aee39-b364-4205-bfea-b4aba03869fa",
     "showTitle": false,
     "title": "--i18n-c1553692-dd61-427f-81be-48078744aa78"
    }
   },
   "source": [
    "\n",
    "\n",
    "### Manipulate Datetimes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd53b02b-85c5-469a-82e7-65d375388a88",
     "showTitle": false,
     "title": "--i18n-6dcbf443-fe08-43b7-8869-33f2af780fb1"
    }
   },
   "source": [
    "\n",
    "\n",
    "#### **`date_add`**\n",
    "Returns the date that is the given number of days after start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bb3cb8e-35f1-4840-bf5d-969dc9a2ff73",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_add\n",
    "\n",
    "plus_2_df = timestamp_df.withColumn(\"plus_two_days\", date_add(col(\"timestamp\"), 2))\n",
    "# display(plus_2_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c4b45c0-6012-4e28-bca8-1205dd4df916",
     "showTitle": false,
     "title": "--i18n-693676fb-cd44-466d-905c-6b620b68abfc"
    }
   },
   "source": [
    "\n",
    "\n",
    "### Clean up classroom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33754ad5-0473-42fc-b8ee-03d95c70478c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting the learning environment:\n| dropping the schema \"muaazkhurshid_q1yg_da_asp\"...(1 seconds)\n| removing the working directory \"dbfs:/mnt/dbacademy-users/muaazkhurshid@uni-koblenz.de/apache-spark-programming-with-databricks\"...(0 seconds)\n\nValidating the locally installed datasets:\n| listing local files...(4 seconds)\n| validation completed...(4 seconds total)\n"
     ]
    }
   ],
   "source": [
    "DA.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02609478-7333-4ade-bb4e-952808f9f600",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "&copy; 2023 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/>\n",
    "<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "ASP 3.2 - Datetimes",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
